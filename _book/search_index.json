[["statistical-models.html", "Chapter 8 Statistical Models 8.1 Log-likelihood and numerical underflow", " Chapter 8 Statistical Models 8.0.1 A First Statistical Model (This will be a somewhat clumsy model, and we’ll develop a better one later.) The main aim of a generative model is to describe how the data we observed is generated. Let’s create a simple model of observed word order, with the following assumptions - It follows from this model that the following observations have the following probabilities: recipient observation  (default word order?) probability animate \\(1\\) \\(p_1\\) animate \\(0\\) \\(1-p_1\\) inanimate \\(1\\) \\(p_2\\) inanimate \\(0\\) \\(1-p_2\\) Doesn’t this mean that we could even compute the is the probability of the data, if we only knew \\(p_1\\), and \\(p_2\\)? Absolutely! Lets call our vector of observations \\((y_1, y_2, y_3, \\ldots, y_n)\\). Given specific values for the parameters \\(p_1, p_2\\), the probability of each individual observation of \\(P(y_n|p_1,p_2)\\) can be read is given in the above table. If we can compute the probability of one observation, we should be able to compute the probability of a whole data set. Remember the joint probability of two independent events \\(A\\), \\(B\\)? It’s \\(P(A) \\cdot P(B)\\). … and because our generative process described by the graphic above assumes independence of observations (i.e., it doesn’t assume dependence), we can compute the the probability of the data set as: \\[P(y_1,~y_2,~...,~y_n|p_1,~p_2) = P(y_1|p_1, p_2) \\cdot P(y_2|p_1, p_2)\\cdot ... \\cdot P(y_n|p_1,p_2)\\] Let’s write an R function which will compute the probability of each data point, and one that will compute the probability of the data set. compute_prob_observation &lt;- function(is_rec_animate, is_default_wo, p1, p2) { # make sure we&#39;re dealing with a single observation, i.e., vector length == 1 stopifnot(length(is_default_wo) == 1 &amp;&amp; length(is_rec_animate) == 1) # different courses of action for animate and inanimate recipients if (is_rec_animate) { # I&#39;ll use if/else, but nested ifelse() calls will work just as well prob_of_observation &lt;- ifelse(is_default_wo, p1, 1-p1) } else { # i.e., if is_rec_animate is FALSE (i.e., !is_rec_animate is TRUE) prob_of_observation &lt;- ifelse(is_default_wo, p2, 1-p2) } prob_of_observation # this what the function returns } Let’s test it: compute_prob_observation(is_rec_animate = 1, is_default_wo = 1, p1 = .1, p2 = .7) ## [1] 0.1 OK. compute_prob_observation(is_rec_animate = 1, is_default_wo = 0, p1 = .1, p2 = .7) ## [1] 0.9 OK. compute_prob_observation(is_rec_animate = 0, is_default_wo = 1, p1 = .1, p2 = .7) ## [1] 0.7 OK. compute_prob_observation(is_rec_animate = 0, is_default_wo = 0, p1 = .1, p2 = .7) ## [1] 0.3 OK. It seems to do what it’s supposed to. Now let’s write a function that does it for two vectors, one vector of observations and one of animacy values: # We&#39;ll need to use sapply, which runs through a vector of elements and calls a function for each element # For example: sapply(1:10, print) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 ## [1] 1 2 3 4 5 6 7 8 9 10 compute_probs_all_observations &lt;- function(is_rec_animate, is_default_wo, p1, p2) { # make sure the input vectors are of equal length stopifnot(length(is_default_wo) == length(is_rec_animate) ) # create a function which computes the probability of the i-th observation compute_prob_ith_observations &lt;- function(i) { compute_prob_observation(is_rec_animate = is_rec_animate[i], is_default_wo = is_default_wo[i], p1 = p1, p2 = p2) } # iterate over all positions in the vectors and compute the probability of # the i-th observation on each iteration # (sapply iterates over a vector) sapply(1:length(is_default_wo), compute_prob_ith_observations) } Let’s try it out on some artificial data: # Let&#39;s take a look at the output of the function, with (rather extreme) made-up data, ... # ... sample size 10 per group n &lt;- 5 # 10 1-s, and 10 0-s is_rec_animate &lt;- c(rep(1,n), rep(0,n)) # only default word orders for animate recipients, only non-default for the others is_default_wo &lt;- c(rep(1,n), rep(0,n)) synth_df &lt;- data.frame(is_rec_animate, is_default_wo) View(synth_df) # compute the probability for the first probs &lt;- compute_probs_all_observations(is_rec_animate, is_default_wo, p1 = .5, p2 = .5) How likely are such extreme data under \\(p_1=0.5\\) and \\(p_2=0.5\\)? # are data like this unlikely under p1=0.5 and p2=0.5? probs ## [1] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 Not very unlikely. What about the entire dataset? prod(probs) ## [1] 0.0009765625 Quite “unlikely”, but that’s because many numbers \\(&lt;1\\) were multiplied. Would other parameters do any better? Apparently some values for \\(p_1\\) and \\(p_2\\) are better than others. Can you tell from the plot what they are? 8.1 Log-likelihood and numerical underflow Now, let’s apply this logic to the entire dataset. First, let’s check for \\(p_1=0.5\\) and \\(p_2=0.5\\). library(languageR) # bring the observations into a usable format dative %&lt;&gt;% mutate(is_default_wo = (RealizationOfRecipient == &quot;NP&quot;) ) dative %&lt;&gt;% mutate(is_rec_animate = (AnimacyOfRec == &quot;animate&quot;) ) # compute the probability for the first probs &lt;- compute_probs_all_observations(dative$is_rec_animate, dative$is_default_wo, p1 = .5, p2 = .5) prod(probs) ## [1] 0 Zero? Whaaaaaaaat is going on? Might the fact below be relevant? nrow(dative) ## [1] 3263 The product of many numbers between \\(0\\) and \\(1\\) is going to be practically \\(0\\): \\(0.1\\cdot0.1=0.01\\), \\(0.1\\cdot0.1\\cdot0.1=0.001\\), \\(0.1\\cdot0.1\\cdot0.1\\cdot0.1=0.0001\\). This results in numerical underflows. (Computers only have a limited precision.) 8.1.1 Logarithm Only logarithms can help us now: \\(log_b(x) = y\\), when \\(b^y = x\\) (with some details omitted) In other words, the logarithm of \\(x\\) to base \\(b\\) is \\(y\\), which is the number by which you need to exponentiate \\(b\\) to get \\(x\\) For example, \\(log_{10}(100) = 2\\), and \\(10^2 = 100\\). Makes sense? So how will this help us? The logarithm maps numbers between \\(0\\) and \\(1\\) onto the range \\(-\\infty\\) and \\(0\\), i.e. from \\((0;1]\\) to \\((-\\infty; 0]\\). Thus, we can use half the number line just to express numbers between \\(0\\) and \\(1\\) (i.e., probabilities). Multiplication of regular probabilities corresponds to addition of log-probabilities: \\[log_b(x \\cdot y) = log_b(x) + log_b(y)\\] Let’s take a look at this function (\\(log_{e}\\)): 8.1.2 Log-likelihood All this means that we can get the log-probability of the entire dataset by summing all individual log-probabilities \\[log_b(p_1 \\cdot p_2 \\cdot \\ldots \\cdot p_n ) = log_b(p_1) + log_b(p_2) + \\ldots log_b(p_n)\\] Let’s use this to compute a log-likelihood surface for \\(p_1\\) and \\(p_2\\) for the dative data set. The red point marks the maximum likelihood (i.e., the ‘peak’ likelihood). Note that the likelihood profile is much more flat along the \\(p_2\\) axis. Below is are cross-sections of the above plot, with the red point marking the maximum likelihood estimates for \\(p_1\\) and \\(p_2\\). Let’s take a look at why the likelihood profile for \\(p_2\\) is more flat than for \\(p_1\\): It’s an issue of sample size. There are many fewer instances of inanimate recipients (first row) than animate recipients (second row). The flat likelihood profile for \\(p_2\\) tells us that while the parameter which is the most likely to produce the observed data is \\(0.48\\) (which is also the observed proportion \\(\\hat{p_2}\\)), the data isn’t all that much less likely under other values for \\(p_2\\) (for example, \\(0.45\\), \\(.5\\), \\(.52\\)). is_rec_animate perc_default_wo N FALSE 0.4811715 239 TRUE 0.7602513 3024 Cool. So what can we do with this? Well, for one, we can compute the relative likelihood of the data for two sets of parameters. Remember that what we have plotted above is the log-likelihood of the data given the parameters, i.e. \\(log~P(D|p_1, p_2)\\) This means that: \\[P(D|p_1=\\theta_1, p_2=\\theta_1)/P(D|p_1=\\theta&#39;_1, p_2=\\theta&#39;_1),\\] because \\(e^{log(a)} = a\\), since \\(log(x)\\) and \\(e^x\\) are inverses of each other: \\[e^{log( P(D|p_1=\\theta_1, p_2=\\theta_1)/P(D|p_1=\\theta&#39;_1, p_2=\\theta&#39;_1) )},\\] and because \\(log(a/b) = log(a) - log(b)\\): \\[e^{log( P(D|p_1=\\theta_1, p_2=\\theta_1)) - log(P(D|p_1=\\theta&#39;_1, p_2=\\theta&#39;_1) )}\\] Great, now we can quantify for any parameter value \\(\\theta&#39;\\), how much less likely the data are for that parameter value, than for the maximum likelihood estimate \\(\\hat{\\theta}\\). We can even calculate an interval around \\(\\hat{\\theta}\\), such that all \\(\\theta&#39;\\)s for which the data is considered too unlikely, falls outside of it. For a ratio of \\(0.5\\), that interval is roughly \\([.75;.77]\\) for \\(p_1\\), and \\([.44;.52]\\) for \\(p_2\\). But instead, we could ask ourselves, is \\(P(D|p_1,p_2)\\) really what we want? Do we really want the probability of the data given certain parameters? In my opinion, it’s not. We are actually interested in \\(P(p_1, p_2|D)\\). And \\(P(D|p_1,p_2)\\) is just a crude proxy for it. The problem is that one cannot maintain a frequentist view of probability and use the expression \\(P(p_1, p_2|D)\\) … "]]
