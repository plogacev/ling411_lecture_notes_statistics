---
output:
  pdf_document: default
  html_document: default
---
# Scales of Measurement{#scalesOfMeasurement}
*(an adapted version of Danielle Navarro's [chapter 2](https://learningstatisticswithr.com/).)*

As might recall, the outcome of a measurement is what we call a **_variable_**. But not all variables are of the same qualitative type, and it’s very useful to understand what types there are.
A very useful concept for distinguishing between different types of variables is what’s known as scales of measurement.

## Nominal scale

- A **_nominal scale_** variable (also referred to as a **_categorical variable_**) is one in which there is no particular
relationship between the different possibilities. 
- *No ordering*: For these kinds of variables it doesn’t make any sense to say that one of them is “bigger’ or “better” than any other one, and it doesn’t make any sense to average them.

Examples:

- The classic example for this is *“eye color”*. Eyes can be *blue, green* and *brown*, among other possibilities, but none of them is any “better” than any other one. As a result, it would feel really odd to talk about an “average eye color”.
- Similarly, gender is nominal too: male isn’t better or worse than female, neither does it make sense to try to talk about an “average gender”. 
- In short, nominal scale variables are those for which the only thing you can say about the different possibilities is that they are different. That’s it.

- Let’s take a slightly closer look at this. Suppose I was doing research on how people commute to and from work. One variable I would have to measure would be what kind of transportation people use to get to work. This “transport type” variable could have quite a few possible values, including: “train”, “bus”, “car”, “bicycle”, etc. For now, let’s suppose that these four are the only possibilities, and suppose that when I ask 100 people how they got to work today, and I get this:

```{r echo=FALSE}
knitr::kable(rbind(
              c( "(1) Train", 12),
              c( "(2) Bus", 30),
              c( "(3) Car", 48),
              c( "(4) Bicycle", 10)
              ),
              col.names = c("Transportation", "Number of people"),
  booktabs = TRUE)
```

- So, what’s the average transportation type? Obviously, the answer here is that there isn’t one. It’s a silly question to ask. You can say that travel by car is the most popular method, and travel by train is the least popular method, but that’s about all. 

<!--
Similarly, notice that the order in which I list the options isn’t very interesting. I could have chosen to display the data like this

```{r echo=FALSE}
knitr::kable(rbind(
              c( "(3) Car", 48),
              c( "(1) Train", 12),
              c( "(4) Bicycle", 10),
              c( "(2) Bus", 30)
              ),
              col.names = c("Transportation", "Number of people"),
  booktabs = TRUE)
```

and nothing really changes.
-->

## Ordinal scale

- **_Ordinal scale_** variables have a bit more structure than nominal scale variables, but not by a lot. An
ordinal scale variable is one in which there is a natural, meaningful way to *order* the different possibilities, but you can’t do anything else. 

Examples:

- The usual example given of an ordinal variable is “finishing position in a race”. You can say that the person who finished first was faster than the person who finished second, but you don’t know how much faster. As a consequence we know that 1st < 2nd, and we know that 2nd < 3rd, but the difference between 1st and 2nd might be much larger than the difference between 2nd and 3rd.
- Here’s an more interesting example. Suppose I’m interested in people’s attitudes to climate change, and I ask them to pick one of these four statements that most closely matches their beliefs:

(1) Temperatures are rising, because of human activity
(2) Temperatures are rising, but we don’t know why
(3) Temperatures are rising, but not because of humans
(4) Temperatures are not rising

- These four statements actually do have a natural ordering, in terms of *“the extent to which they agree with the current science”*. 
  * Statement 1 is a close match, 
  * Statement 2 is a reasonable match, 
  * Statement 3 isn't a very good match, 
  * Statement 4 is in strong opposition to the science. 

- So, in terms of the thing I’m interested in (the extent to which people endorse the science), I can order the items as $1 > 2 > 3 > 4$.

- So, let’s suppose I asked 100 people these questions, and got the following answers:

```{r echo=FALSE}
knitr::kable(rbind(
              c( "(1) Temperatures are rising, because of human activity", 51),
              c( "(2) Temperatures are rising, but we don’t know why", 20),
              c( "(3) Temperatures are rising, but not because of humans", 10),
              c( "(4) Temperatures are not rising", 19)
              ),
              col.names = c("Response", "Number"),
  booktabs = TRUE)
```

- When analysing these data, it seems quite reasonable to try to group (1), (2) and (3) together, and say that 81 of 100 people were willing to at least partially endorse the science. 
- And it’s also quite reasonable to group (2), (3) and (4) together and say that 49 of 100 people registered at least some disagreement with the dominant scientific view. 
- However, it would be entirely bizarre to try to group (1), (2) and (4) together and say that 90 of 100 people said \ldot what? There’s nothing sensible that allows you to group those responses together at all.

<!--
- Notice that while we can use the natural ordering of these items to construct sensible
groupings, what we can’t do is average them. For instance, in my simple example here, the “average”
response to the question is 1.97. If you can tell me what that means, I’d love to know. Because that
sounds like gibberish to me!
-->

## Interval scale

- In contrast to nominal and ordinal scale variables, **_interval scale_** and ratio scale variables are variables for which the numerical value is genuinely meaningful. In the case of interval scale variables, the differences between the numbers are interpretable, but the variable doesn’t have a “natural” zero value.

Examples:

- A good example of an interval scale variable is measuring temperature in degrees celsius. For instance, if it was 15°  yesterday and 18°  today, then the 3°  difference between the two is genuinely meaningful.
Moreover, that 3°  difference is exactly the same as the 3°  difference between 7°  and 10°. In short, addition and subtraction are meaningful for interval scale variables. *(Nevermind that a change of 10° will feel different at 20° compared to 30°.)*
  * However, notice that the 0° does not mean “no temperature at all”: it actually means “the temperature
at which water freezes”, which is pretty arbitrary. As a consequence, it becomes pointless to try to
multiply and divide temperatures. It is wrong to say that 20° is twice as hot as 10°, just as it is weird and meaningless to try to claim that 20° is negative two times as hot as 10°.

- Or suppose I’m interested in looking at how the attitudes of first-year university students have changed over time. Obviously, I’m going to want to record the year in which each student started. This is an interval scale variable. A student who started in 2003 did arrive 5 years before a student who started in 2008. However, it would be completely insane for me to divide 2008 by 2003 and say that the second student started “1.0024 times later” than the first one.
That doesn’t make any sense at all.

## Ratio scale

- The final type of variable to consider is a **_ratio scale variable_**, in which zero really means zero, and it’s okay to multiply and divide. 

Example:

- A good example of a ratio scale variable is *response time (RT)*. In a lot of tasks it’s very common to record the amount of time somebody takes to solve a problem or answer a question, because it’s an indicator of how difficult the task is.
  * Suppose that Alan takes 2.3 seconds to respond to a question, whereas Ben takes 3.1 seconds. 
  * As with an interval scale variable, addition and subtraction are both meaningful here. 
  * Ben really did take $3.1 - 2.3 = 0.8$ seconds longer than Alan did. 
  * Notice that multiplication and division also make sense here too: Ben took $3.1/2.3 = 1.35$ times as long as Alan did to answer the question. 
  * The reason why you can do this is that, for a ratio scale variable such as RT, “zero seconds” really does mean “no time at all”.

```{r echo=FALSE}
knitr::kable(rbind(
              c( "Nominal scale",  "",  "", ""),
              c( "Ordinal scale",  "x", "", ""),
              c( "Interval scale", "x", "x", ""),
              c( "Ratio scale",    "x", "x", "x")
              ),
              col.names = c("Scale", "Meaningful order", "Meaningful differences", "Meaningful ratio"),
  booktabs = TRUE)
```


## Continuous versus discrete variables

- There’s a second kind of distinction that you need to be aware of, regarding what types of variables you
can run into. This is the distinction between continuous variables and discrete variables. The difference
between these is as follows:
  * A **_continuous variable_** is one in which, for any two values that you can think of, it’s always
  logically possible to have another value in between.
  * A **_discrete variable_** is, in effect, a variable that isn’t continuous. For a discrete variable, it’s
  sometimes the case that there’s nothing in the middle.

- For exampl, response time is continuous. If Alan takes 3.1 seconds and Ben takes 2.3 seconds to respond
to a question, then it’s possible for Cameron’s response time to lie in between, by taking 3.0 seconds.
And of course it would also be possible for David to take 3.031 seconds to respond, meaning that his RT
would lie in between Cameron’s and Alan’s. And while in practice it might be impossible to measure
RT that precisely, it’s certainly possible in principle. *Because we can always find a new value for RT in
between any two other ones, we say that RT is continuous.*

- Discrete variables occur when this rule is violated. For example, nominal scale variables are always
discrete: there isn’t a type of transportation that falls “in between” trains and bicycles, not in the strict
mathematical way that 2.3 falls in between 2 and 3. So transportation type is discrete. 
- Similarly, ordinal scale variables are always discrete: although “2nd place” does fall between “1st place” and “3rd place”, there’s nothing that can logically fall in between “1st place” and “2nd place”. 
- Interval scale and ratio scale variables can go either way. As we saw above, response time (a ratio scale variable) is continuous.
- *Temperature* in degrees celsius (an interval scale variable) is also continuous. 
- However, the *year you went to school* (an interval scale variable) is discrete. There’s no year in between 2002 and 2003. 
- The *number of questions you get right* on a true-or-false test (a ratio scale variable) is also discrete: since a true-or-false question doesn’t allow you to be “partially correct”, there’s nothing in between 5/10 and 6/10. 


## Some complexities

- Okay, I know you’re going to be shocked to hear this, but \ldots the real world is much messier than
this little classification scheme suggests. Very few variables in real life actually fall into these nice neat
categories, so you need to be kind of careful not to treat the scales of measurement as if they were
hard and fast rules. It doesn’t work like that: they’re guidelines, intended to help you think about the
situations in which you should treat different variables differently. Nothing more.

- So let’s take a classic example, maybe the classic example, of a psychological measurement tool: the
Likert scale. The humble Likert scale is the bread and butter tool of all survey design. You yourself
have filled out hundreds, maybe thousands of them, and odds are you’ve even used one yourself. Suppose
we have a survey question that looks like this:


Which of the following best describes your opinion of the statement that “all pirates are
freaking awesome” \ldots

and then the options presented to the participant are these:

(1) Strongly disagree
(2) Disagree
(3) Neither agree nor disagree
(4) Agree
(5) Strongly agree

This set of items is an example of a 5-point **_Likert scale_**: people are asked to choose among one of
several (in this case 5) clearly ordered possibilities, generally with a verbal descriptor given in each case.
However, it’s not necessary that all items be explicitly described. This is a perfectly good example of a
5-point Likert scale too:

(1) Strongly disagree
(2)
(3)
(4)
(5) Strongly agree

Likert scales are very handy, if somewhat limited, tools. The question is, what kind of variable are
they? They’re obviously discrete, since you can’t give a response of 2.5. They’re obviously not nominal
scale, since the items are ordered; and they’re not ratio scale either, since there’s no natural zero.
But are they ordinal scale or interval scale? One argument says that we can’t really prove that the
difference between “strongly agree” and “agree” is of the same size as the difference between “agree”
and “neither agree nor disagree”. In fact, in everyday life it’s pretty obvious that they’re not the same
at all. So this suggests that we ought to treat Likert scales as ordinal variables. On the other hand, in
practice most participants do seem to take the whole “on a scale from 1 to 5” part fairly seriously, and
they tend to act as if the differences between the five response options were fairly similar to one another.
As a consequence, a lot of researchers treat Likert scale data as if it were interval scale. It’s not interval
scale, but in practice it’s close enough that we usually think of it as being **_quasi-interval scale_**.

# Descriptive statistics{#descriptives}
*(an adapted version of Danielle Navarro's [chapter 3/5](https://learningstatisticswithr.com/book/) on descriptive statistics.)*

```{r echo=FALSE, results='hide', message=FALSE}
library(lsr)
library(dplyr)
library(magrittr)
library(ggplot2)

theme_set(theme_bw())
```

- Any time that you get a new data set to look at, one of the first tasks that you have to do is find ways of summarising the data in a compact, easily understood fashion. This is what **_descriptive statistics_** (as opposed to **_inferential statistics_**) is all about. In fact, to many people the term "statistics" is synonymous with descriptive statistics. 

- Before going into any details, let's take a moment to get a sense of why we need descriptive statistics. To do this, let's load a dataset on sleep in mammals.
```{r}
#load( "./data_navarro/aflsmall.Rdata" )
#library(lsr)
#who()

mammalian_sleep <- 
      read.csv("./data/msleep_ggplot2.csv") %>% 
      dplyr::select(name, sleep_total, bodywt) %>%
      dplyr::rename(sleep_total_h = sleep_total, bodywt_kg = bodywt) %>%
      dplyr::mutate(sleep_total_h = round(sleep_total_h) )

head(mammalian_sleep)
```
<!-- data: https://rstudio-pubs-static.s3.amazonaws.com/155770_0c286e39b48b4670ab7ababb103a7448.html -->

- There are three variables here, `name`, `sleep_total_h` and `bodywt_kg`. For each animal named in `name`, the `sleep_total_h` variable contains the average number of hours animals of this kind sleep per day. The variable `bodywt_kg` contains the average weight of that animal in kg. 

- Let's have a look at the `sleep_total_h` variable:
```{r}
#print(afl.margins)
print(mammalian_sleep$sleep_total_h)
```

- This output doesn't make it easy to get a sense of what the data are actually saying. Just "looking at the data" isn't a terribly effective way of understanding data. In order to get some idea about what's going on, we need to calculate some descriptive statistics and draw some nice pictures.
<!--
to-do: Brief explanation of histograms
(Chapter \@ref(graphics). Since the descriptive statistics are the easier of the two topics, I'll start with those, but nevertheless I'll show you a histogram of the `afl.margins` data, since it should help you get a sense of what the data we're trying to describe actually look like. But for what it's worth, this histogram -- which is shown in Figure \@ref(fig:histogram1) -- was generated using the `hist()` function. We'll talk a lot more about how to draw histograms in Section \@ref(hist). For now, it's enough to look at the histogram and note that it provides a fairly interpretable representation of the `afl.margins` data.
-->
- Let's take a look at a **_histogram_** of these data: A **_histogram_** is a graphical representation of the frequencies of different values of ranges of values in the data. 
```{r histogram1, fig.cap="A histogram of the average amount of sleep by animal (the `sleep_total_h` variable). As you might expect, the larger the margin the less frequently you tend to see it.", echo=TRUE}
  mammalian_sleep %>% 
      ggplot(aes(sleep_total_h)) + 
      geom_histogram(binwidth = 1, 
                     color = "black", 
                     fill = "lightgrey")
```

## Measures of central tendency{#centraltendency}

Drawing pictures of the data, as I did in Figure \@ref(fig:histogram1) is an excellent way to convey the "gist" of what the data is trying to tell you, it's often extremely useful to try to condense the data into a few simple "summary" statistics. In most situations, the first thing that you'll want to calculate is a measure of **_central tendency_**. That is, you'd like to know something about the "average" or "middle" of your data lies. The two most commonly used measures are the mean, median and mode; occasionally people will also report a trimmed mean. I'll explain each of these in turn, and then discuss when each of them is useful.

### The mean{#mean}

- The **_mean_** of a set of observations is just a normal, old-fashioned average: add all of the values up, and then divide by the total number of values. The first five animals' typical amount of sleep is 12 + 17 + 14 + 15 + 4, so the mean of these observations is just:
$$
\frac{12 + 17 + 14 + 15 + 4}{5} = \frac{62.4}{5} = 12.48
$$
- Of course, this definition of the mean isn't news to anyone: averages (i.e., means) are used so often in everyday life that this is pretty familiar stuff. However, since the concept of a mean is something that everyone already understands, I'll use this as an excuse to start introducing some of the mathematical notation that statisticians use to describe this calculation, and talk about how the calculations would be done in R. 

- The first piece of notation to introduce is $N$, which we'll use to refer to the number of observations that we're averaging (in this case $N = 5$). 
- Next, we need to attach a label to the observations themselves. It's traditional to use $X$ for this, and to use subscripts to indicate which observation we're actually talking about. 
- That is, we'll use $X_1$ to refer to the first observation, $X_2$ to refer to the second observation, and so on, all the way up to $X_N$ for the last one. Or, to say the same thing in a slightly more abstract way, we use $X_i$ to refer to the $i$-th observation. Just to make sure we're clear on the notation, the following table lists the 5 observations in the `sleep_total_h` variable, along with the mathematical symbol used to refer to it, and the actual value that the observation corresponds to:

<!-- to-do: the sleep data isn't great, because it actually consists of averages itself; moreover they are bounded -->

```{r echo=FALSE}
knitr::kable(rbind(
              c( paste0(mammalian_sleep$name[1], " (animal 1)"), "$X_1$", paste0(mammalian_sleep$sleep_total_h[1], " hours")),
              c( paste0(mammalian_sleep$name[2], " (animal 2)"), "$X_2$", paste0(mammalian_sleep$sleep_total_h[2], " hours")),
              c( paste0(mammalian_sleep$name[3], " (animal 3)"), "$X_3$", paste0(mammalian_sleep$sleep_total_h[3], " hours")),
              c( paste0(mammalian_sleep$name[4], " (animal 4)"), "$X_4$", paste0(mammalian_sleep$sleep_total_h[4], " hours")),
              c( paste0(mammalian_sleep$name[5], " (animal 5)"), "$X_5$", paste0(mammalian_sleep$sleep_total_h[5], " hours"))),
col.names = c("the observation", "its symbol", "the observed value"),
  booktabs = TRUE)

```



- Okay, now let's try to write a formula for the mean. By tradition, we use $\bar{X}$ as the notation for the mean. So the calculation for the mean could be expressed using the following formula:
$$
\bar{X} = \frac{X_1 + X_2 + ... + X_{N-1} + X_N}{N}
$$

- This formula is entirely correct, but it's terribly long, so we make use of the **_summation symbol_** $\scriptstyle\sum$ to shorten it.^[The choice to use $\Sigma$ to denote summation isn't arbitrary: it's the Greek upper case letter sigma, which is the analogue of the letter S in that alphabet. Similarly, there's an equivalent symbol used to denote the multiplication of lots of numbers: because multiplications are also called "products", we use the $\Pi$ symbol for this; the Greek upper case pi, which is the analogue of the letter P.] If I want to add up the first five observations, I could write out the sum the long way, $X_1 + X_2 + X_3 + X_4 +X_5$ or I could use the summation symbol to shorten it to this:
$$
\sum_{i=1}^5 X_i
$$
- Taken literally, this could be read as "the sum, taken over all $i$ values from 1 to 5, of the value $X_i$". But basically, what it means is "add up the first five observations". In any case, we can use this notation to write out the formula for the mean, which looks like this:
$$
\bar{X} = \frac{1}{N} \sum_{i=1}^N X_i 
$$

- In all honesty, I can't imagine that all this mathematical notation helps clarify the concept of the mean at all. In fact, it's really just a fancy way of writing out the same thing I said in words: add all the values up, and then divide by the total number of items. However, that's not really the reason I went into all that detail. 
- My goal was to try to make sure that everyone reading this book is clear on the notation that we'll be using throughout the book: $\bar{X}$ for the mean, $\scriptstyle\sum$ for the idea of summation, $X_i$ for the $i$th observation, and $N$ for the total number of observations. 
- We're going to be re-using these symbols a fair bit, so it's important that you understand them well enough to be able to "read" the equations, and to be able to see that it's just saying "add up lots of things and then divide by another thing".

### Calculating the mean in R

Okay that's the maths, how do we get the magic computing box to do the work for us? If you really wanted to, you could do this calculation directly in R. For the first numbers, do this just by typing it in as if R were a calculator...
```{r}
(12 + 17 + 14 + 15 + 4) / 5

```
... in which case R outputs the answer 36.6, just as if it were a calculator. 

- However, we learned quicker ways of doing that 
```{r}
sum( mammalian_sleep$sleep_total_h[1:5] )/ 5
# or:
mean( mammalian_sleep$sleep_total_h[1:5] )
```



### The median{#median}

- The second measure of central tendency that people use a lot is the **_median_**, and it's even easier to describe than the mean. The median of a set of observations is just the middle value. 
- As before let's imagine we were interested only in the first 5 animals: They sleep 12, 17, 14, 15, and 4 hours respectively. To figure out the median, we sort these numbers into ascending order:
$$
4, 12, \color{red}{14}, 15, 17
$$
- From inspection, it's obvious that the median value of these 5 observations is 32, since that's the middle one in the sorted list (I've put it in red to make it even more obvious). Easy stuff. 

- But what should we do if we were interested in the first 6 animals rather than the first 5? Since the sixth animal sleeps for 14 hours, our sorted list is now: 
$$
4, 12, \color{red}{14}, \color{red}{14}, 15, 17
$$

- That's also easy. It's still 14. 

- But what we do if we were interested in the first 8 animals? Here is our new sorted list.
$$
4,  7,  9, \color{red}{12}, \color{red}{14}, 14, 15, 17
$$
- There are now *two* middle numbers, 12 and 14. The median is defined as the average of those two numbers, which is of course 13.
- To understand why, think of the median as the value that divides the sorted list of numbers into two halves -- those on its left, and those on its right. 

- As before, it's very tedious to do this by hand when you've got lots of numbers. To illustrate this, here's what happens when you use R to sort all the sleep durations. First, I'll use the `sort()` function to display the 83 numbers in increasing numerical order:
```{r}
sort( mammalian_sleep$sleep_total_h )
```
- Because the vector is 83 elements long, the middle value is at position 42. This means that the median of this vector is 10. In real life, of course, no-one actually calculates the median by sorting the data and then looking for the middle value. In real life, we use the median command:
```{r}
median( mammalian_sleep$sleep_total_h )
```
which outputs the median value of 10. 




### Mean or median? What's the difference?


```{r meanmedian, echo=FALSE, fig.cap="An illustration of the difference between how the mean and the median should be interpreted. The mean is basically the \"centre of gravity\" of the data set: if you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation. Half of the observations are smaller, and half of the observations are larger."}
knitr::include_graphics("./img/descriptives2/meanmedian.png")

```


- Knowing how to calculate means and medians is only a part of the story. You also need to understand what each one is saying about the data, and what that implies for when you should use each one. This is illustrated in Figure \@ref(fig:meanmedian) the mean is kind of like the "centre of gravity" of the data set, whereas the median is the "middle value" in the data. What this implies, as far as which one you should use, depends a little on what type of data you've got and what you're trying to achieve. As a rough guide:
 
<!-- to-do: talk about scales
- If your data are nominal scale, you probably shouldn't be using either the mean or the median. Both the mean and the median rely on the idea that the numbers assigned to values are meaningful. If the numbering scheme is arbitrary, then it's probably best to use the mode (Section \@ref(mode)) instead. 
- If your data are ordinal scale, you're more likely to want to use the median than the mean. The median only makes use of the order information in your data (i.e., which numbers are bigger), but doesn't depend on the precise numbers involved. That's exactly the situation that applies when your data are ordinal scale. The mean, on the other hand, makes use of the precise numeric values assigned to the observations, so it's not really appropriate for ordinal data.
- For interval and ratio scale data, either one is generally acceptable. Which one you pick depends a bit on what you're trying to achieve. The mean has the advantage that it uses all the information in the data (which is useful when you don't have a lot of data), but it's very sensitive to extreme values, as we'll see in Section \@ref(trimmedmean).  

Let's expand on that last part a little. 
-->

- One consequence is that there's systematic differences between the mean and the median when the histogram is asymmetric (skewed; see Section \@ref(skewandkurtosis)). This is illustrated in Figure \@ref(fig:meanmedian) notice that the median (right hand side) is located closer to the "body" of the histogram, whereas the mean (left hand side) gets dragged towards the "tail" (where the extreme values are). 
- To give a concrete example, suppose Bob (income \$50,000), Kate (income \$60,000) and Jane (income \$65,000) are sitting at a table: the average income at the table is \$58,333 and the median income is \$60,000. Then Bill sits down with them (income \$100,000,000). The average income has now jumped to \$25,043,750 but the median rises only to \$62,500. If you're interested in looking at the overall income at the table, the mean might be the right answer; but if you're interested in what counts as a typical income at the table, the median would be a better choice here.

<!--
### A real life example{#housingpriceexample}
- Income example
-->



### Trimmed mean{#trimmedmean} 

- One of the fundamental rules of applied statistics is that the data are messy. Real life is never simple, and so the data sets that you obtain are never as straightforward as the statistical theory says.^[Or at least, the basic statistical theory -- these days there is a whole subfield of statistics called *robust statistics* that tries to grapple with the messiness of real data and develop theory that can cope with it.] This can have awkward consequences. To illustrate, consider this rather strange looking data set (nevermind what it represents):
$$
-100,2,3,4,5,6,7,8,9,10
$$
- If you were to observe this in a real life data set, you'd probably suspect that something funny was going on with the $-100$ value. It's probably an **_outlier_**, a value that doesn't really belong with the others. You might consider removing it from the data set entirely, and in this particular case I'd probably agree with that course of action. 
- In real life, however, you don't always get such cut-and-dried examples. For instance, you might get this instead:
$$
-15,2,3,4,5,6,7,8,9,12
$$
- The $-15$ looks a bit suspicious, but not anywhere near as much as that $-100$ did. In this case, it's a little trickier. It *might* be a legitimate observation, it might not.

- When faced with a situation where some of the most extreme-valued observations might not be quite trustworthy, the mean is not necessarily a good measure of central tendency. It is highly sensitive to one or two extreme values, and is thus not considered to be a **_robust_** measure.

- One remedy that we've seen is to use the median. A more general solution is to use a "trimmed mean".  To calculate a trimmed mean, what you do is "discard" the most extreme examples on both ends (i.e., the largest and the smallest), and then take the mean of everything else. The goal is to preserve the best characteristics of the mean and the median: 
  * just like a median, you aren't highly influenced by extreme outliers, but ...
  * like the mean, you "use" more than one of the observations. 
  
- Generally, we describe a trimmed mean in terms of the percentage of observation on either side that are discarded. So, for instance, a 10% trimmed mean discards the largest 10% of the observations *and* the smallest 10% of the observations, and then takes the mean of the remaining 80% of the observations. 
- Not surprisingly, the 0% trimmed mean is just the regular mean, and the 50% trimmed mean is the median. In that sense, trimmed means provide a whole family of central tendency measures that span the range from the mean to the median.


- For our toy example above, we have 10 observations, and so a 10% trimmed mean is calculated by ignoring the largest value (i.e., `12`) and the smallest value (i.e., `-15`) and taking the mean of the remaining values. First, let's enter the data
```{r}
dataset <- c( -15,2,3,4,5,6,7,8,9,12 )
```
Next, let's calculate means and medians:
```{r}
mean( dataset )

median( dataset )

```

- That's a fairly substantial difference, but I'm tempted to think that the mean is being influenced a bit too much by the extreme values at either end of the data set, especially the $-15$ one. So let's just try trimming the mean a bit. If I take a 10% trimmed mean, we'll drop the extreme values on either side, and take the mean of the rest: 
```{r}
mean( dataset, trim = .1)

```

- In this case it gives exactly the same answer as the median. Note that, to get a 10% trimmed mean you write `trim = .1`, not `trim = 10`. 

  
### Mode{#mode}
- The **_mode_** is the last measure of central tendency we'll look at. It is very simple: it is the value that occurs most frequently. 
- Let's look at the some soccer data: specifically, the European Cup and Champions League results in the time from 1955-2016.

- Lets find out which team has won the most matches. The command below tells R we just want the first 25 rows of the data.frame.
```{r}
library(engsoccerdata)

table(champs$tiewinner) %>% sort(decreasing=T) %>% .[1:25]

```
- It appears that the mode of the winning team is 'Real Madrid'. That doesn't come as a surprise even to me.
- Of course, the mode is the right (and only) summary for nominal variables.

- But we can compute a mode for all types of variables. For example, let's take a look at the mean, median, and mode of the total number of goals per game.

```{r}
champs %<>% mutate(total_goals = hgoal + vgoal) # total goals is home team goals + visitor goals
```

```{r}
mean(champs$total_goals)
```

```{r}
median(champs$total_goals)
```

```{r}
modeOf(champs$total_goals)
```

```{r}
ggplot(champs, aes(total_goals)) + geom_histogram()
```

## Summary

- There are multiple measures of central tendency that can be used to summarize an aspect of a distribution: **_ (arithmetic) mean, median, and mode_**.
- They answer different questions about distribution. For example, in the distribution of number of goals per game in the previous section
  * mean: "If the same number of goals were scored in each game, how many goals would be scored?"
  * median: "What is a 'mediocre' game like?"
  * mode: "What is the most typical game like?"
<!--
mean: "If all the income were redistributed equally among the households, how much would each household make?"
median: "What is the income of the middle household?"
mode: "If we choose a household at random, what is its income most likely to be?"
trimmed mean: "What is the mean income of those who have work? (i.e., how much *can* one expect to earn *if* one finds a job)"
-->



